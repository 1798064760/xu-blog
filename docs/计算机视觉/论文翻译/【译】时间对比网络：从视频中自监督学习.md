# 【译】时间对比网络：从视频中自监督学习


**摘要**:我们提出了一种完全从多角度记录的未标记视频中学习表征和机器人行为的自监督方法，并研究了这种表征如何用于两种机器人模仿场景:模仿人类视频中的物体交互和模仿人类姿态。模仿人类行为需要一个视图不变的表征，它捕获末端执行器(手或机器人手爪)与环境、对象属性和身体姿势之间的关系。我们使用度量学习损失来训练我们的表征，其中相同观测的多个同时视角被吸引到嵌入空间中，同时被时间邻居排斥，这些时间邻居通常在视觉上相似，但功能上不同。换句话说，这个模型同时学习识别不同图像之间的共同点，以及相似图像之间的不同点。这个信号使我们的模型发现属性，这些属性不会随视角而改变，但会随时间而改变，而忽略了讨厌的变量，如遮挡、运动模糊、光照和背景。我们证明了这种表征可以被机器人直接模仿人类的姿势，而不需要一个明确的对应关系，并且它可以在强化学习算法中用作奖励函数。虽然表征是从一组未标记的与任务相关的视频中学习的，但机器人的行为，如pouring，则是通过观看一个人类的3d演示来学习的。通过跟随人类演示下学到的表征获得的奖励函数使强化学习在现实机器人系统的实际应用中更有效。视频结果，开源代码和数据集在 sermanet.github.io/imitate

## I. 介绍

虽然监督学习已经在一系列任务上取得了成功，比如对象分类等，但在机器人等交互式应用程序中出现的许多问题难以监督。例如，将浇注任务的每个方面都标记得足够详细以使机器人能够理解所有与任务相关的属性，这是不切实际的。浇注演示可以根据背景、容器和视角的不同而有所不同，在每一帧中可能有许多突出的属性，例如，一只手是否接触容器，容器的倾斜，或目标容器中当前的液体量或其粘度。理想情况下，现实世界中的机器人能够做到两件事:纯粹通过观察来学习对象交互任务的相关属性，以及理解如何将人类的姿势和对象交互映射到机器人上，以便直接通过第三人称视频观察进行模仿


![图1](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig1.png)

图1： 时间对比网络(Time-Contrastive Networks, TCN):同时从不同视点拍摄的 Anchor(锚) 和 positive（正例） 图像在嵌入空间上尽量靠近，而从同一序列不同时间拍摄的 negative (负例)图像尽量远离。模型通过同时回答以下问题来训练自己:不同外观的蓝色框架之间有什么共同之处?相似的红色和蓝色镜框有什么不同?由此产生的嵌入可以用于一般的自监督机器人，但也可以自然地处理3d人的模仿。

在这项工作中，我们采取了一个步骤，通过使用自监督和多视角表征学习来同时解决这些挑战。我们从未标记的交互场景多视角视频中获得学习信号，如图1所示。通过对多视角视频的学习，所学习的表征有效地将位姿等功能属性分离出来，同时保持视角和agent不变。然后，我们展示了机器人如何通过强化学习或直接回归学习将这种视觉表征与相应的运动指令联系起来，从而通过观察人类有效地学习新任务。

我们工作的主要贡献是一个表征学习算法,基于现有的语义相关特征(在我们的例子中,特征来自ImageNet数据集[1,2]上训练的网络)来产生一个对对象交互和姿势敏感，对不关心的视角和外观不敏感的度量嵌入。我们证明了这种表征形式可以用来创建一个奖励函数来强化机器人技能的学习，只使用原始的视频演示来监督，并直接模仿人类的姿势，没有任何明确的关联级别的对应，同样直接来自原始视频。我们的实验证明了使用真实的机器人进行浇筑任务的有效学习，在模拟中移动盘子进出碗碟架，以及对人类姿势的实时模仿。虽然在我们的实验中，我们为每个任务训练了不同的TCN嵌入，但是在未来的工作中我们将根据不同上下文中的各种演示构建嵌入，并讨论如何构建更大的多任务嵌入。

## II. 相关工作
**模仿学习**:模仿学习[3]被广泛用于从专家演示中学习机器人技能[4,5,6,7]，它可以分为两个领域:行为克隆和反强化学习(IRL)。行为克隆考虑一个监督学习问题，其中行为的例子作为状态-动作对提供[8,9]。另一方面，IRL使用专家演示来学习一个奖励函数，该函数可用于优化带有强化学习[10]的模仿策略。这两种类型的模仿学习都需要专家在与学习者相同的情境中提供示范。在机器人技术中，这可能是通过动觉(kinesthetic)演示[11]或远程操作[12]来实现的，但这两种方法都需要相当多的操作者专业知识。如果我们的目标是赋予机器人广泛的行为技能，那么能够直接从人类的第三人称视频中获得这些技能将会大大提高可扩展性。最近，一系列的工作研究了在不同的背景下观察到的一个演示的模仿问题，例如从一个不同的角度或一个具有不同化身的代理，例如 a  human[13,14,15] Liu等人提出在专家语境和学习者语境之间翻译示范，通过尽量缩短翻译示范的距离来学习模仿策略。然而，Liu等人明确排除了任何具有域转移的演示，即演示是由一个人执行，并由具有明显视觉差异的机器人模仿(例如，人手vs.机器人钳子)。与此相反，我们的TCN模型是在具有不同实施例、对象和背景的各种演示上进行训练的。这使得我们基于TCN的方法可以直接模拟人类演示，包括人类将液体倒入杯子的演示，以及模拟人类的姿势，而不需要任何显式的关节水平对齐。据我们所知，我们的工作是第一个模拟原始视频演示的方法，既可以模拟原始视频，又可以处理人与机器人之间的领域转换.

**无标签训练信号**:无标签学习的视觉表征承诺，可以从无监督的数据中进行视觉理解，因此，近年来已被广泛探索。在这一领域之前的工作已经研究了无监督学习，将其作为一种从小型标记数据集[17]、图像检索[18]和各种其他任务中进行监督学习的方法[19、20、21、22]。在本文中，我们特别关注表征学习，以实现对象、人及其环境之间的模型交互，这需要隐式建模广泛的因素，如功能关系，同时不受讨厌的变量(如视角和外观)的影响。该方法利用多视角同时记录的信号构造图像嵌入。之前的许多工作都使用了多种模式和时间或空间的一致性(连贯性)来提取嵌入和特征。例如，[23,24]利用视频中声音和视觉线索的同时出现来学习有意义的视觉特征。[20]还提出了一种通过训练网络进行跨通道输入重建的多通道自监督方法。[25,26]利用图像的空间连贯性作为自监督信号，[27]利用运动线索进行自监督分割任务。这些方法更侧重于空间关系，它们提供的非监督信号是对本文所探讨的方法的补充。

之前的许多工作都使用了时间连贯性[28,29,30,31]。其他人也使用度量学习训练视角不变性[22,32,33]。我们工作的新颖之处在于将这两个对立的方面结合起来，如 III-A 节所述。[19]使用了一个三元损失，在嵌入过程中，跟踪序列的第一帧和最后一帧会更接近，而来自其他视频的随机负帧则相距很远。我们的方法的不同之处在于，我们用暂时的邻居作为负例来对抗同时刻的另一个视角锚定的正例。这导致我们的方法发现有意义的维度，如属性或姿态，而[19]侧重于学习类内不变性。同步多视角捕获还提供了准确的对应，而跟踪不提供，并可以提供一套丰富的对应，如闭塞，模糊，照明和视角。

也有研究提出使用预测作为学习信号[34,35]。结果表征通常主要基于预测图像的真实性进行评估，这仍然是一个具有挑战性的开放问题。许多其他先前的方法已经使用了各种各样的标签和先验来学习嵌入。[36]使用标记数据集来训练一个姿态嵌入，然后从训练数据中为一个姿态检索任务找到新图像的最近邻居。我们的方法是通过ImageNet训练初始化的，但是可以发现像姿态和任务进程(例如浇注任务)这样的维度，而不需要任何特定于任务的标签。[37]探索各种类型的物理先验，如物体在重力下的轨迹，以学习在没有明确监督下的物体跟踪。我们的方法在精神上是相似的，因为它使用了时间共现（temporal co-occurrence），这是一个普遍的物理属性，但我们使用的原则是通用的和广泛适用的，不需要具体的物理规则的任务输入。


![图2](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig2.png)

图2：**多视图捕捉** 配备了智能手机的两个操作员。自由移动相机会带来丰富多样的比例、视角、遮挡、运动模糊和两个相机之间的背景对应


**镜像神经元**:人类和动物已经被实验证明，在他们的环境[38]中拥有物体和其他物体的视角不变表象，而众所周知的关于“镜像神经元”的工作已经证明，这些视角不变表象对于模仿[39]至关重要。图2中的多视角捕获设置类似于[38]使用的实验设置，而我们的机器人模仿设置(机器人模仿人类运动，但从未接收到ground truth位姿标签)考察了在一个学习系统中如何产生自监督位姿识别

## III. 利用时间对比网络进行模仿

我们模仿学习的方法是只依赖来自世界的感官输入。我们通过两个步骤实现这一点。首先，我们从被动观察中学习抽象表征。其次，我们利用这些表征来引导机器人模仿人类的行为，并学习执行新的任务。我们使用“模仿”（imitation）这个术语，而不是“演示”(demonstrations)，因为我们的模型也从对非演示行为的被动观察中学习。机器人需要对它所看到的一切有一个大致的了解，以便更好地识别一个积极的演示。我们故意坚持只使用自监督来保持该方法在现实世界中的可扩展性。在这项工作中，我们探讨了几种利用时间作为信号进行无监督表征学习的方法。下面我们还将探讨实现自监督机器人控制的不同方法。

A.训练时间对比网络

我们在图1中说明了我们的时间对比(TC)方法。该方法通过三元损失[40]实现多视图度量学习。图像$x$的嵌入由$f(x)\in \mathbb{R}^d$表示。这种损失保证了同时出现的$x_i^a$(anchor)和$x_i^p$(positive)在嵌入空间上比任何图像$x_i^n$(negative)更接近。因此，我们的目标是学习一个这样的嵌入$f$:

$$
||f(x_i^a)-f(x_i^p)||_2^2 + \alpha < ||f(x_i^a)-f(x_i^n)||_2^2 \\\\
\forall (f(x_i^a),f(x_i^p),f(x_i^an)) \in \mathcal{T}
$$

其中$\alpha$是正例和负例对之间的一个强制的间隔，$\mathcal{T}$是训练集中所有可能的三元组。核心理念是两个坐标系(锚和正例)来自相同的时间,但不同的视角(或模式)拉在一起,而时间邻居上的视觉相似帧被分开。这个信号有两个目的:学习无标签的解缠表征，同时学习视角不变性以进行模仿。交叉试图对应鼓励学习视角，规模，遮挡，运动模糊，照明和背景的不变性，因为正例和锚沿这些因素的变化，显示相同的主题。例如，除了遮挡之外，图1展示了顶部和底部序列之间的所有这些转换。除了学习一组丰富的视觉不变性外，我们还对用于模仿的第三人称到第一人称一致的视角不变性感兴趣。时间对比信号如何导致解缠表征?它通过引入时间上的邻居之间的竞争来解释视觉上随时间的变化。例如，在图1中，由于邻居在视觉上是相似的，区分它们的唯一方法是建模杯子里的液体量，或者建模手或物体的姿势和它们之间的相互作用。另一种对TCN提供的强大的训练信号的理解方式是同时识别模型上的两个约束条件:沿着图1中的视图轴，模型学习来解释看起来不同的图像之间什么是相同的,而沿着时间轴学会解释看起来相似的图像物体之间有什么不同。请注意，虽然模仿这种方法的自然能力很重要，但是用无监督的方式学习丰富的表征是更重要的贡献。我们的方法的关键因素是，多种视角为物理世界的变化奠定了基础，消除了可能的解释的歧义。我们在第四章中指出，TCN确实可以在没有监督的情况下发现不同物体或物体之间的对应关系，以及杯子和浇铸阶段的液体量等属性。这是一个有点令人惊讶的发现，因为从来没有提供过物体或物体之间的明确对应关系。我们假设不同但功能相似的对象在嵌入空间中的流形自然对齐，因为它们共享一些功能和外观。

多视图数据收集非常简单，只需两个配备智能手机的操作人员即可捕获，如图2所示。一个操作人员在执行任务时保持对感兴趣区域的固定视角，而另一个操作人员自由移动摄像机以引入上面讨论的变化。虽然多视图捕获比单视图捕获更麻烦，但我们认为，与人工标记等替代方法相比，多视图捕获便宜、简单、实用。

![图3](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig3.png)

图3：**单视角TCN**:在锚点周围的小窗口中选择正例，在相同的序列上，从远处的时间步选择负例

我们也可以考虑单视角视频训练的时间对比模型，如图3所示。在这种情况下，在锚的一定范围内随机选择正例帧。给定正例范围，再计算出一个间隔范围。在间隔范围边缘外随机选择负例，对模型进行训练。我们根据经验选择了2倍的正例范围，它本身设置为 0.2s。虽然我们在第四节中展示了多视图TCN的最佳性能，但是当没有多视图数据可用时，单视图版本仍然很有用。


B.用强化学习学习机器人行为

在这项工作中，我们考虑了一个模仿学习场景，其中的演示来自一个不同于学习代理的化身——第3人称视频观察，例如机器人模仿一个人。由于上下文的差异，直接跟踪所演示的像素值并不能提供学习模仿行为的合理方法。如前一节所述，TCN嵌入提供了一种提取图像特征的方法，这些特征不受摄像机角度和操作对象的影响，并且可以解释世界上的物理交互。我们利用这种洞察力来构建一个基于人类视频演示的TCN嵌入与机器人摄像机记录的摄像机图像之间的距离的奖励函数。如第IV-B节所示，通过反复试验优化这个奖励函数，我们能够模拟机器人演示的行为，仅利用其视觉输入和视频演示进行学习。虽然我们使用多个多视角视频来训练TCN，但是视频演示只包含一个人从随机角度执行任务的单个视频。

设$V=(v_1,\cdots,v_T)$是视频演示序列中每一帧的TCN嵌入。对于机器人任务执行过程中,其观察到的每个相机图像，我们计算TCN嵌入$W=(w1,\cdots,w_T)$。我们定义了一个奖励函数$R(v_t,w_t)$基于欧几里德距离的平方和一个 Huber-style 损失:

$$
R(v_t,w_t) = -\alpha ||w_t-v_t||_2^2-\beta\sqrt{\gamma+||w_t-v_t||_2^2}
$$

其中$\alpha$和$\beta$是权重参数(经验选择)和$\gamma$是一个小的常数。当嵌入相距更远时欧氏距离的平方($\alpha$加权)给了我们更强的梯度,从而在学习开始时导致更大的策略更新。当嵌入向量变得非常近的时候，Huber-style 损失($\beta$加权)开始占据主要作用以确保高精度任务的执行和接近训练结束时的微调。

为了学习机器人的模仿策略，我们利用强化学习优化了上述的奖励函数。特别地，为了优化机器人的轨迹，我们使用了PILQR算法[41]。该算法通过LQR将近似的基于模型的更新与拟合的时变线性动力学和无模型修正相结合。我们注意到，在我们的任务中，TCN嵌入在机器人面前提供了一个表现良好的低维(在我们的实验中是32维)视觉世界状态。通过在系统状态中包含TCN特性(即状态=关节角度+关节速度+TCN特性)，我们可以在基于模型的LQR更新过程中利用动力学的线性近似，从而显著加速训练速度。强化学习设置的细节可以在附录B中找到。

C.直接模仿人体姿势

在前一节中，我们讨论了如何将强化学习与TCNs一起使用，从而直接从人类的视频演示中学习对象交互技能。在本节中，我们将介绍使用TCNs的另一种方法:直接模仿人体姿态。虽然对象交互技能主要需要匹配演示的功能方面，但直接的姿态模仿需要学习人类和机器人姿态之间的隐式映射，因此涉及到框架之间更细粒度的关联。一旦学会，人-机器人映射，则可以通过初始化一个接近解决方案的策略来加速RL的探索阶段。


![图4](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig4.png)

图4：**姿势模仿训练信号**:时间对比、自回归和人类监督。时间对比信号让模型学习了人类或机器人个体的丰富表征。自回归允许机器人根据自身的图像来预测自己的关节。人类监督信号是从试图模仿机器人姿势的人那里收集来的。

我们通过自回归来学习直接的姿势模仿。如图4和图8所示，为自监督的人体姿态模仿。这个想法是直接预测机器人的内部状态给它一个自己的图像。类似于在镜子中看着自己，机器人可以将自己对自身形象的预测回归到自身的内部状态。我们首先通过观察人类和机器人执行随机动作来训练一个共享的TCN嵌入。然后机器人通过自回归来训练自己。因为它使用的TCN嵌入在人类和机器人之间是不变的，机器人可以在自我训练后自然地模仿人类。因此，我们得到了一个可以对人类动作进行端对端模仿的系统，尽管它从未被赋予任何人类姿态标签，也没有人对机器人的对应。我们在图4中展示了一种收集端到端模仿的人工监督的方法。然而，与时间对比信号和自回归信号相反，人工监控信号的采集是非常嘈杂和昂贵的。我们用它来作为我们在第IV-C节中的方法的基准，并表明大量廉价的监督可以有效地与少量昂贵的监督混合在一起.

## IV. 实验

我们的实验旨在研究三个问题。首先，我们检查TCN是否可以学习更能表示对象交互属性的视觉表征，例如浇注任务中的阶段。这使得我们可以比较TCN和其他自监督的表征形式。其次，我们研究如何将TCN与强化学习结合起来，在仿真和真实机器人平台上获得复杂的对象操作技能。最后，我们证明了TCN可以使机器人连续、实时地模仿人类的姿势，而不需要明确指定机器人和人类之间的任何关节级别的对应。总之，这些实验说明了TCN表征在建模姿态、对象交互以及机器人模仿者和人类演示者之间的隐式对应方面的适用性。

A. 从通用表征中发现属性

1. 液体浇铸:在这个实验中，我们通过观察一个人从不同的容器中浇铸液体到不同的杯子中，研究TCN简单地捕捉到了什么。这些视频是用两种标准的智能手机拍摄的(见图2)，一种是人类从主角度拍摄的，另一种是自由移动的第三人称视角。通过一个现成的应用程序在两部手机之间同步捕获，每个序列大约5秒长。我们将收集到的多视角序列分为3组:133个序列用于训练(总计约11分钟)，17个序列用于验证，30个序列用于测试。训练视频包含透明和不透明的杯子，但我们将测试视频限制为透明杯子，以评估模型是否了解杯子有多满。

2. 模型:在所有后续的实验中，我们使用了一个源自Inception架构[2]的类似于[42]的自定义架构。它由Inception模型直到层“Mixed_5d”(用ImageNet预训练的权值初始化)，然后是2个卷积层，一个空间softmax层[42]和一个全连接层。嵌入是一个全连接层，在我们的自定义模型上添加了32个单元。这种嵌入是训练要么与多视角TC损失，单视角TC损失，或打乱&学习损失[31]。对于TCN模型，我们使用未经修改的[40]的三元损失，其间隔值为0.2。请注意，在所有的实验中，负例总是来自于与正例相同的序列。我们还用其他度量学习损失，即 npairs [43]和 lifted structured[44]进行了实验，结果表明它们具有可比性。在接下来的实验中，我们使用Imagenet预训练的Inception模型[1,2](一个2048维的向量)的分类之前的最后一层的输出作为基线，称之为“Inception-ImageNet”。由于自定义模型是由ImageNet预训练初始化的，因此它是一个自然的比较点，允许我们控制通过ImageNet训练而不是其他方法引入的任何不变性。我们将TCN模型与根据我们的数据训练的 shuffle & learn基线进行比较，使用与论文相同的超参数(tmax 60, tmin 15，负类占比0.75)。注意，在我们的实现中，shuffle & learn基线和TCN都不能从高动态帧的偏置采样中获益。为了研究多视角和单视角之间的区别，我们比较了单视角TCN，它的正例范围为0.2秒，负例为2倍。

3. 模型选择:非监督训练中出现的模型选择问题。您应该根据验证损失来选择最佳模型吗?或者为一个给定的任务手工标记一个小的验证?我们报告两种方法的数据。在表I中，我们根据模型的最低验证损失来选择每个模型，而在表IV中，我们根据前面描述的5个属性标记的小验证集的分类得分来选择模型。与预期一样，通过验证分类得分选择的模型在分类任务中表现得更好。然而，由loss选择的模型，除了shuffle & learn之外，其表现稍差，而shuffle & learn的准确率损失更大。我们的结论是，选择基于验证损失的TCN模型是合理的，而不是使用任何标签。
4. 训练时间:在表IV中，我们观察到多视角TCN(使用三元损失)的性能优于单视角模型，同时需要的训练时间减少了15倍，并且在完全相同的数据集上进行训练。我们的结论是，利用时间的相似性（correspondences）可以大大提高训练的时间和准确性。

![表1](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/tab1.png)

表1：浇注对齐度和分类误差:所有模型的验证损失最小。分类误差考虑了与浇注有关的5类，详见表2。


![表2](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/tab2.png)

表2：详细属性分类误差，用验证损失进行模型选择。

5. 定量评估:我们在表1中提出了两个度量标准来评估模型能够捕获的内容。对齐度度量一个模型在语义上如何对齐两个视频。分类指标衡量的是一个模型能够在多大程度上分解与浇注相关的属性，这在实际的机器人浇注任务中是很有用的。本节中的所有结果都使用嵌入空间中的最近邻进行评估。给定一个视频的每一帧，每个模型必须选择另一个视频中语义最相似的帧。“随机”基线只是从第二个视频返回一个随机帧

序列对齐度量在学习模仿时特别相关和重要，特别是从第三方的角度来看。为每个浇注测试视频,一个操作员标注相对应的关键帧到以下事件:用手接触浇注容器的第一帧,液体流动的第一帧,液体流动的最后一帧,用手接触容器的最后一帧。这些关键帧建立了一个粗略的语义对齐，应该提供所有视频之间一个相对准确的分段线性对应。对于测试集中任意一对视频$(v_1,v_2)$，我们嵌入每一帧让模型进行评估。对于源视频$v_1$的每一帧，我们将其与来自$v_2$的所有帧的嵌入空间中最近的相邻帧相关联。我们评估$v_2$中最近的近邻在语义上与$v_1$中的参照帧是否一致。通过标记对齐，我们找到了参照帧与目标视频$v_2$的比例位置，并计算到该位置的帧距离，根据目标段长度归一化。

我们在测试和验证集中标记以下属性来评估表2中报告的分类任务:手是否与容器接触?(是或否);容器是否在容器的浇注距离内?(是或否);浇注容器的倾斜角是多少?(值90、45、0和-45度);液体在流动吗?(是或否);收件器是否含有液体?(是或否)。之所以要评估这些特殊属性，是因为它们对于模拟和执行浇注任务很重要。分类结果按类分布归一化。请注意，虽然这可以与引论中提到的监督分类器进行比较，但是在实际应用中，例如在机器人技术中，期望每个可能的任务都有标签是不现实的。相反，在这项工作中，我们的目标是比较现实的一般 off-the-shelf（现成的,不用定制的）模型，它可以使用不需要新的标签。

在表1中，我们发现多视角TCN模型优于所有基线。我们观察到，单视角TCN和shuffle & learn在分类指标上是平等的，而在对齐指标上则不是。我们发现，与其他基线相比，一般的现成的先启（Inception,起初）特征明显表现不佳。附录C提供了嵌入的定性示例和t-SNE可视化演示。我们鼓励读者参考补充视频以更好地掌握这些结果。

B. 学习对象交互技能


![图5](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig5.png)

图5：模拟盘架任务。左:第三人称VR演示的盘架任务。中间:训练中机器人摄像头的视角。右图:执行摆盘任务的机器人


![图6](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig6.png)

图6：真实的机器人浇注任务。左:浇注任务的第三人称人体演示。中间:训练中机器人摄像头的视角。右图:机器人执行浇注任务。

在本节中，我们使用第III-B节中描述的基于 TCN 的奖励函数，通过强化学习从第三人称演示中学习机器人模仿行为。我们在两个任务上评估了我们的方法，一个是在模拟的盘架环境下的盘子转移(图5，使用 Bullet 物理引擎[45])，另一个是真实的机器人从人类演示中浇注(图6)。

1. 任务设置:模拟的盘架环境包括两个放在桌子上的架子和装满盘子的架子。这项任务的目标是将盘子从一个架子移到另一个架子而不让它们掉下来。这需要一个复杂的运动，有多个阶段，如伸手，抓取，拿起，携带和放置。我们使用虚拟现实(VR)系统来记录人类的演示，以操纵一个自由浮动的钳子并移动盘子(图5左)。我们通过在虚拟世界中放置第一视角和第三人称摄像头来记录VR演示的视频。除了演示之外，我们还记录了一系列随机运动，以增加我们的TCN模型的泛化能力。在记录演示之后，我们将一个模拟的 7-DoF KUKA 机器人手臂放置在碗碟架环境中(图5右侧)，并在其上附加一个第一视角摄像机。然后利用机器人摄像机图像(图5中)计算TCN奖励函数。利用随机高斯噪声初始化机器人策略。

对于真实的机器人浇注任务，我们首先从多个摄像头收集多视角数据来训练TCN模型。这套训练集包括人类在智能手机摄像头上录制的倒液体的视频，以及机器人在两个机器人摄像头上录制的倒颗粒状珠子的视频。我们不仅收集了手头任务的正例演示，我们还收集了实际上不涉及倒东西的各种交互作用，如移动杯子、将杯子打翻、洒出珠子等，以覆盖机器人可能需要理解的事件范围。浇注实验分析了TCNs如何隐式地建立人与机器人对物体的操作之间的对应关系。我们用来训练TCN的数据集包括执行浇铸任务的约20分钟人类视频，以及以浇铸以外的方式操作杯子和瓶子的约20分钟人类视频，比如移动杯子、把它们打翻等等。为了使TCN能够同时表示人臂和机器人臂，并隐式地将它们对应起来，还必须向TCN提供数据，使其能够理解机器人臂的外观。为此，我们添加了一组数据，其中包括机器人手臂在类似浇筑的设置中操纵杯子的约20分钟视频。注意，这些数据本身不一定能说明成功的浇筑任务:在强化学习期间跟踪的最后一次演示包括一个人成功地浇筑一杯液体，而机器人则用橙色的珠子完成浇筑任务。然而，我们发现提供一些以机器人手臂为特色的剪辑对于TCN获得一个能够正确记录人类和机器人之间的相似性的表现形式是很重要的。使用额外的机器人数据在这里是合理的，因为在从未见过自己手臂的情况下，期望机器人做得好是不现实的。然而，随着时间的推移，学习的任务越多，所需的任务就越少。虽然TCN接受了大约1小时的与浇注相关的多视角序列的训练，但是机器人的策略只能从一个人提供的单个液体浇注视频中学习(图6左)。在这个视频中，我们训练了一个 7-DoF KUKA 机器人来完成如图6(右)所示的粒状珠子的浇注。我们从机器人摄像机图像(图6中)计算TCN嵌入，并使用随机高斯噪声初始化机器人策略。我们在手腕关节处设置了更高的初始探索，因为它对浇注运动的贡献最大(对于所有比较的算法)。



---
**参考**：
1. 论文：Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine [Time-Contrastive Networks: Self-Supervised Learning from Video](https://arxiv.org/abs/1704.06888)
