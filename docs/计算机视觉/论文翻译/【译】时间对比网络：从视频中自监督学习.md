# 【译】时间对比网络：从视频中自监督学习


**摘要**:我们提出了一种完全从多角度记录的未标记视频中学习表征和机器人行为的自监督方法，并研究了这种表征如何用于两种机器人模仿场景:模仿人类视频中的物体交互和模仿人类姿态。模仿人类行为需要一个视图不变的表征，它捕获末端执行器(手或机器人手爪)与环境、对象属性和身体姿势之间的关系。我们使用度量学习损失来训练我们的表征，其中相同观测的多个同时视角被吸引到嵌入空间中，同时被时间邻居排斥，这些时间邻居通常在视觉上相似，但功能上不同。换句话说，这个模型同时学习识别不同图像之间的共同点，以及相似图像之间的不同点。这个信号使我们的模型发现属性，这些属性不会随视角而改变，但会随时间而改变，而忽略了讨厌的变量，如遮挡、运动模糊、光照和背景。我们证明了这种表征可以被机器人直接模仿人类的姿势，而不需要一个明确的对应关系，并且它可以在强化学习算法中用作奖励函数。虽然表征是从一组未标记的与任务相关的视频中学习的，但机器人的行为，如pouring，则是通过观看一个人类的3d演示来学习的。通过跟随人类演示下学到的表征获得的奖励函数使强化学习在现实机器人系统的实际应用中更有效。视频结果，开源代码和数据集在 sermanet.github.io/imitate

## I. 介绍

虽然监督学习已经在一系列任务上取得了成功，比如对象分类等，但在机器人等交互式应用程序中出现的许多问题难以监督。例如，将浇注任务的每个方面都标记得足够详细以使机器人能够理解所有与任务相关的属性，这是不切实际的。浇注演示可以根据背景、容器和视角的不同而有所不同，在每一帧中可能有许多突出的属性，例如，一只手是否接触容器，容器的倾斜，或目标容器中当前的液体量或其粘度。理想情况下，现实世界中的机器人能够做到两件事:纯粹通过观察来学习对象交互任务的相关属性，以及理解如何将人类的姿势和对象交互映射到机器人上，以便直接通过第三人称视频观察进行模仿


![图1](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig1.png)

图1： 时间对比网络(Time-Contrastive Networks, TCN):同时从不同视点拍摄的 Anchor(锚) 和 positive（正例） 图像在嵌入空间上尽量靠近，而从同一序列不同时间拍摄的 negative (负例)图像尽量远离。模型通过同时回答以下问题来训练自己:不同外观的蓝色框架之间有什么共同之处?相似的红色和蓝色镜框有什么不同?由此产生的嵌入可以用于一般的自监督机器人，但也可以自然地处理3d人的模仿。

在这项工作中，我们采取了一个步骤，通过使用自监督和多视角表征学习来同时解决这些挑战。我们从未标记的交互场景多视角视频中获得学习信号，如图1所示。通过对多视角视频的学习，所学习的表征有效地将位姿等功能属性分离出来，同时保持视角和agent不变。然后，我们展示了机器人如何通过强化学习或直接回归学习将这种视觉表征与相应的运动指令联系起来，从而通过观察人类有效地学习新任务。

我们工作的主要贡献是一个表征学习算法,基于现有的语义相关特征(在我们的例子中,特征来自ImageNet数据集[1,2]上训练的网络)来产生一个对对象交互和姿势敏感，对不关心的视角和外观不敏感的度量嵌入。我们证明了这种表征形式可以用来创建一个奖励函数来强化机器人技能的学习，只使用原始的视频演示来监督，并直接模仿人类的姿势，没有任何明确的关联级别的对应，同样直接来自原始视频。我们的实验证明了使用真实的机器人进行浇筑任务的有效学习，在模拟中移动盘子进出碗碟架，以及对人类姿势的实时模仿。虽然在我们的实验中，我们为每个任务训练了不同的TCN嵌入，但是在未来的工作中我们将根据不同上下文中的各种演示构建嵌入，并讨论如何构建更大的多任务嵌入。

## II. 相关工作
**模仿学习**:模仿学习[3]被广泛用于从专家演示中学习机器人技能[4,5,6,7]，它可以分为两个领域:行为克隆和反强化学习(IRL)。行为克隆考虑一个监督学习问题，其中行为的例子作为状态-动作对提供[8,9]。另一方面，IRL使用专家演示来学习一个奖励函数，该函数可用于优化带有强化学习[10]的模仿策略。这两种类型的模仿学习都需要专家在与学习者相同的情境中提供示范。在机器人技术中，这可能是通过动觉(kinesthetic)演示[11]或远程操作[12]来实现的，但这两种方法都需要相当多的操作者专业知识。如果我们的目标是赋予机器人广泛的行为技能，那么能够直接从人类的第三人称视频中获得这些技能将会大大提高可扩展性。最近，一系列的工作研究了在不同的背景下观察到的一个演示的模仿问题，例如从一个不同的角度或一个具有不同化身的代理，例如 a  human[13,14,15] Liu等人提出在专家语境和学习者语境之间翻译示范，通过尽量缩短翻译示范的距离来学习模仿策略。然而，Liu等人明确排除了任何具有域转移的演示，即演示是由一个人执行，并由具有明显视觉差异的机器人模仿(例如，人手vs.机器人钳子)。与此相反，我们的TCN模型是在具有不同实施例、对象和背景的各种演示上进行训练的。这使得我们基于TCN的方法可以直接模拟人类演示，包括人类将液体倒入杯子的演示，以及模拟人类的姿势，而不需要任何显式的关节水平对齐。据我们所知，我们的工作是第一个模拟原始视频演示的方法，既可以模拟原始视频，又可以处理人与机器人之间的领域转换.

**无标签训练信号**:无标签学习的视觉表征承诺，可以从无监督的数据中进行视觉理解，因此，近年来已被广泛探索。在这一领域之前的工作已经研究了无监督学习，将其作为一种从小型标记数据集[17]、图像检索[18]和各种其他任务中进行监督学习的方法[19、20、21、22]。在本文中，我们特别关注表征学习，以实现对象、人及其环境之间的模型交互，这需要隐式建模广泛的因素，如功能关系，同时不受讨厌的变量(如视角和外观)的影响。该方法利用多视角同时记录的信号构造图像嵌入。之前的许多工作都使用了多种模式和时间或空间的一致性(连贯性)来提取嵌入和特征。例如，[23,24]利用视频中声音和视觉线索的同时出现来学习有意义的视觉特征。[20]还提出了一种通过训练网络进行跨通道输入重建的多通道自监督方法。[25,26]利用图像的空间连贯性作为自监督信号，[27]利用运动线索进行自监督分割任务。这些方法更侧重于空间关系，它们提供的非监督信号是对本文所探讨的方法的补充。

之前的许多工作都使用了时间连贯性[28,29,30,31]。其他人也使用度量学习训练视角不变性[22,32,33]。我们工作的新颖之处在于将这两个对立的方面结合起来，如 III-A 节所述。[19]使用了一个三元损失，在嵌入过程中，跟踪序列的第一帧和最后一帧会更接近，而来自其他视频的随机负帧则相距很远。我们的方法的不同之处在于，我们用暂时的邻居作为负例来对抗同时刻的另一个视角锚定的正例。这导致我们的方法发现有意义的维度，如属性或姿态，而[19]侧重于学习类内不变性。同步多视角捕获还提供了准确的对应，而跟踪不提供，并可以提供一套丰富的对应，如闭塞，模糊，照明和视角。

也有研究提出使用预测作为学习信号[34,35]。结果表征通常主要基于预测图像的真实性进行评估，这仍然是一个具有挑战性的开放问题。许多其他先前的方法已经使用了各种各样的标签和先验来学习嵌入。[36]使用标记数据集来训练一个姿态嵌入，然后从训练数据中为一个姿态检索任务找到新图像的最近邻居。我们的方法是通过ImageNet训练初始化的，但是可以发现像姿态和任务进程(例如浇注任务)这样的维度，而不需要任何特定于任务的标签。[37]探索各种类型的物理先验，如物体在重力下的轨迹，以学习在没有明确监督下的物体跟踪。我们的方法在精神上是相似的，因为它使用了时间共现（temporal co-occurrence），这是一个普遍的物理属性，但我们使用的原则是通用的和广泛适用的，不需要具体的物理规则的任务输入。


![图2](/assets/images/计算机视觉/时间对比网络_从视频中自监督学习/fig2.png)

图2：**多视图捕捉** 配备了智能手机的两个操作员。自由移动相机会带来丰富多样的比例、视角、遮挡、运动模糊和两个相机之间的背景对应


**镜像神经元**:人类和动物已经被实验证明，在他们的环境[38]中拥有物体和其他物体的视角不变表象，而众所周知的关于“镜像神经元”的工作已经证明，这些视角不变表象对于模仿[39]至关重要。图2中的多视角捕获设置类似于[38]使用的实验设置，而我们的机器人模仿设置(机器人模仿人类运动，但从未接收到ground truth位姿标签)考察了在一个学习系统中如何产生自监督位姿识别

## III. 利用时间对比网络进行模仿

我们模仿学习的方法是只依赖来自世界的感官输入。我们通过两个步骤实现这一点。首先，我们从被动观察中学习抽象表征。其次，我们利用这些表征来引导机器人模仿人类的行为，并学习执行新的任务。我们使用“模仿”（imitation）这个术语，而不是“演示”(demonstrations)，因为我们的模型也从对非演示行为的被动观察中学习。机器人需要对它所看到的一切有一个大致的了解，以便更好地识别一个积极的演示。我们故意坚持只使用自监督来保持该方法在现实世界中的可扩展性。在这项工作中，我们探讨了几种利用时间作为信号进行无监督表征学习的方法。下面我们还将探讨实现自监督机器人控制的不同方法。

A.训练时间对比网络

我们在图1中说明了我们的时间对比(TC)方法。该方法通过三元损失[40]实现多视图度量学习。图像$x$的嵌入由$f(x)\in \mathbb{R}^d$表示。这种损失保证了同时出现的$x_i^a$(anchor)和$x_i^p$(positive)在嵌入空间上比任何图像$x_i^n$(negative)更接近。因此，我们的目标是学习一个这样的嵌入$f$:

$$
||f(x_i^a)-f(x_i^p)||_2^2 + \alpha < ||f(x_i^a)-f(x_i^n)||_2^2 \\\\
\forall (f(x_i^a),f(x_i^p),f(x_i^an)) \in \mathcal{T}
$$



---
**参考**：
1. 论文：Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine [Time-Contrastive Networks: Self-Supervised Learning from Video](https://arxiv.org/abs/1704.06888)
