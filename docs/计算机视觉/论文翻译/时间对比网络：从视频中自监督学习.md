# 时间对比网络：从视频中自监督学习 (翻译)


**摘要** 我们提出 self-supervised learning 表示 方法 和 机器人 的 行为 完全 从 多个 viewpoints, unlabeled 视频 记录 和 研究
如何在两个机器人的模仿中使用这种表示法
设置:模仿人类视频中的物体交互，
模仿人类的姿势。模仿人类行为
需要一个捕获
末端执行器(手或机器人手爪)之间的关系
环境，对象属性，身体姿势。我们训练
我们的表示法使用了一个度量学习损失，其中有多个
同一观测的同时视点被吸引
在嵌入空间中，同时与时间相排斥
通常在视觉上相似但功能上相似的邻居
不同。换句话说，模型同时学习
识别不同图像之间的共同点，
相似的图像有什么不同。这
信号导致我们的模型发现不需要的属性
改变观点，但要随着时间而改变
忽略讨厌的变量，如遮挡，运动模糊，
灯光和背景。We 证明 可以 使用 此 representation 直接 通过 一 个 机器人 模仿 人类 的 姿势
没有明确的对应关系，并且可以作为
强化学习算法中的奖励函数。
而表示是从未标记的集合中学习的
在与任务相关的视频中，机器人的行为，如倒水是
通过观看由a
人类。通过跟随人类而获得的奖励功能
学习表示下的演示使effi-成为可能
对现实世界有实际意义的强化学习
机器人系统。视频结果，开源代码和数据集
可以在sermanet.github.io/ mimic中找到
---
**参考**：
1. 论文：Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine [Time-Contrastive Networks: Self-Supervised Learning from Video](https://arxiv.org/abs/1704.06888)
