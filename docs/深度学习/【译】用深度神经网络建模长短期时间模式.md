# 【译】用深度神经网络建模长短期时间模式

**摘要**: 多变量时间序列预测是一个涉及多个领域的重要机器学习问题，包括太阳能电站能量输出、电力消耗和交通堵塞的预测。在这些实际应用中产生的时态数据通常涉及长期和短期模式的混合，因此自回归模型和高斯过程等传统方法可能会失败。在这篇论文中，我们提出了一个新的深度学习框架，即长短期时间序列网络(LSTNet)，来解决这个开放的挑战。LSTNet使用卷积神经网络(CNN)和递归神经网络(RNN)来提取变量间的短期局部依赖模式，并发现时间序列趋势的长期模式。此外，我们利用传统的自回归模型来处理神经网络模型的尺度不敏感问题。在我们对具有复杂的重复模式混合的真实数据的评估中，LSTNet比几种最先进的基线方法取得了显著的性能改进。所有的数据和实验代码都可以在网上找到。

**关键词**
Multivariate Time Series, Neural Network, Autoregressive models

**ACM参考格式**:
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Mod- eling Long- and Short-Term Temporal Patterns with Deep Neural Networks. In Proceedings of ACM Conference (SIGIR’18). ACM, New York, NY, USA, 11 pages. https://doi.org/10.475/123_4

## 1. 介绍

多元时间序列数据在我们的日常生活中是无处不在的，从股票市场的价格，高速公路的交通流量，太阳能发电厂的输出，不同城市的温度，等等。在这种应用程序中，用户往往对根据时间序列信号的历史观察预测新趋势或潜在危险事件感兴趣。例如，可以根据几小时前预测的交通堵塞模式设计更好的路线计划，通过预测近期的股票市场可以获得更大的利润。

![图1](/assets/images/深度学习/用深度神经网络建模长短期时间模式/fig1.png)

图1：旧金山湾区道路每小时的占用率，为期两星期

多元时间序列预测经常面临一个主要的研究挑战，即如何捕获和利用多变量之间的动态依赖关系。具体地说，现实世界的应用常常需要短期和长期重复模式的混合，如图1所示，它绘制了高速公路的每小时占用率。显然，有两个重复的模式，每天和每周。前者描绘了早晨和晚上的高峰，而后者反映了工作日和周末的模式。一个成功的时间序列预测模型应该捕捉这两种重复出现的模式，以便进行准确的预测。再举一个例子，考虑一个任务，根据不同位置的大型传感器测量到的太阳辐射，预测太阳能发电厂的输出。长期模式反映了昼夜、夏季和冬季等的差异，而短期模式反映了云层移动、风向变化等的影响。同样，如果不考虑这两种周期性特征，精确的时间序列预测是不可能的。然而，传统的方法如大量的自回归方法[2,12,22,32,35]在这方面存在不足，因为它们大多数不能区分这两种模式，也不能显式和动态地对它们的交互进行建模。针对时间序列预测中现有方法的局限性，我们提出了一个利用深度学习研究最新进展的新框架。

深度神经网络在相关领域得到了深入的研究，并对一系列问题的解决产生了非凡的影响。以[9]为例，递归神经网络(RNN)模型是近年来自然语言处理(NLP)研究的热点。特别是RNN的两个变种,即长期短期记忆(LSTM)[15]和门控递归单元(GRU)[6],显著提高了最先进的机器翻译,语音识别和其他NLP任务的性能，根据输入文档中词语之间的长短期依赖性，有效地捕获词语的含义[1，14，19]。另一个例子是在计算机视觉领域，卷积神经网络(CNN)模型[19,21]通过从输入图像中成功提取不同粒度级别的局部和平移不变特征(有时称为 shapelets )，显示出了出色的性能。

深度神经网络在时间序列分析中也越来越受到重视。以前的大部分工作都集中在时间序列分类上，即时间序列分类。类标签自动分配到时间序列输入的任务。例如，RNN体系结构被研究用于从医疗序列数据中提取信息模式[5,23]，并根据诊断类别对数据进行分类。RNN也被应用于移动数据，根据动作或活动[13]对输入序列进行分类。CNN模型也被用于动作/活动识别[13,20,31]，用于从输入序列中提取平移不变的局部模式作为分类模型的特征。

深度神经网络也被研究用于时间序列预测[8,33]，即，使用过去观测到的时间序列来预测未来范围上未知的时间序列的任务——范围越大，问题越难。这方面的工作包括早期使用朴素RNN模型[7]和结合ARIMA[3]和多层感知器(MLP)的混合模型[16,34,35]的工作，以及最近在时间序列预测[8]中使用的普通RNN和动态玻尔兹曼机的组合。

在本文中，我们提出了一个用于多元时间序列预测的深度学习框架，即长短期时间序列网络(LSTNet)，如图2所示。它利用卷积层发现多维输入变量之间的局部依赖模式，及递归层来捕获复杂的长期依赖模式。另外，利用输入时间序列信号的周期性特性，设计了一种叫递归跳跃(Recurrent-skip)的新的递归结构，用于捕获非常长期的依赖模式，使优化变得更容易。最后，LSTNet将传统的自回归线性模型与非线性神经网络部分并行，使得非线性深度学习模型对具有尺度变化的时间序列具有更强的鲁棒性。在实际季节时间序列数据集的实验中，我们的模型始终优于传统的线性模型和GRU递归神经网络。

本文的其余部分组织如下。第2节概述了相关的背景，包括代表性的自回归方法和高斯过程模型。第3节描述我们提出的LSTNet。第4节报告了我们的模型与真实世界数据集的强基线的比较的评估结果。最后，我们在第5节中总结我们的发现。

## 2. 相关背景

最著名的单变量时间序列模型之一是自回归综合移动平均(ARIMA)模型。ARIMA模型的流行是由于它的统计特性和模型选择过程中著名的Box-Jenkins方法[2]。ARIMA模型不仅能够适应各种指数平滑技术[25]，而且还具有足够的灵活性，可以包含其他类型的时间序列模型，包括自回归(AR)、移动平均(MA)和自回归移动平均(ARMA)。然而，由于ARIMA模型具有较高的计算成本，因此很少用于高维多变量时间序列预测。

另一方面，向量自回归(VAR)由于其简单性，可以说是多元时间序列中应用最广泛的模型[2,12,24]。VAR模型很自然地将AR模型扩展到多元设置，忽略了输出变量之间的依赖关系。近年来，各种VAR模型取得了显著进展，包括用于重尾时间序列的椭圆型VAR模型[27]和用于更好地解释高维变量之间依赖关系的结构化VAR模型[26]，等等。然而，VAR的模型容量随时间窗口大小线性增长，随变量数量二次增长。这意味着，当处理长期的时间模式时，继承的大型模型容易出现过拟合。为了缓解这一问题，[32]提出将原始的高维信号简化为低维隐藏的表征，然后应用VAR进行多种正则化选择的预测。

时间序列预测问题也可以看作是具有时变参数的标准回归问题。因此，不同损失函数和正则化项的回归模型应用于时间序列预测任务也就不足为奇了。例如,线性支持向量回归(SVR)(4、17)在超参数 $\epsilon$ 控制预测误差的阈值的基础下学习最大间隔超平面。岭回归是另一个例子,可以通过设置$\epsilon$为0，重新覆盖来自SVR的模型。最后，[22]应用了 LASSO 模型来鼓励模型参数的稀疏性，从而可以显示不同输入信号之间的有趣模式。由于在机器学习领域中有高质量的现成解算器(off-the-shelf solvers)，这些线性方法实际上对多元时间序列预测更高效。然而，与VARs一样，这些线性模型可能无法捕捉多元信号简复杂的非线性关系，从而导致性能低下，并以牺牲效率为代价。

高斯过程（Gaussian Processes,GP）是对连续函数域上的分布进行建模的一种非参数方法。这与参数化函数类(如VARs和SVRs)定义的模型形成了对比。GP可应用于[28]中提出的多元时间序列预测任务，并可作为贝叶斯推理中函数空间上的先验。例如，对于非线性状态空间模型，[10]提出了一种具有GP先验的完全贝叶斯方法，能够捕获复杂的动态现象。然而，高斯过程的能力是以高计算复杂度为代价的。由于核矩阵的矩阵求逆，多元时间序列预测的高斯过程的一个直接实现具有观测数的立方复杂度。

## 3. 框架

在本节中，我们首先制定时间序列预测问题，然后在接下来的部分中讨论所提议的LSTNet结构的细节(图2)。最后介绍了目标函数和优化策略。

![图2](/assets/images/深度学习/用深度神经网络建模长短期时间模式/fig2.png)

图2：长短期时间序列网络（LSTNet）概览


### 3.1 问题公式化

本文主要研究多元时间序列预测问题。更正式地说，给定一系列完全观测到的时间序列信号$Y=\{y_1,y_2,\cdots,y_T\}$ 其中 $y_t \in \mathbb{R}^n$,$n$是变量的维度，我们的目标是以滚动预测的方式预测一系列的未来信号。也就是说，我们假设$\{y_1,y_2,\cdots,y_T\}$可用，来预测$y_{T+h}$,其中$h$是当前时间戳之前的理想视界，同样的，我们假定$\{y_1,y_2,\cdots,y_T,y_{T+1}\}$可用，来预测下一个时间戳$y_{T+h+1}$的值。我们由此得到在时间戳$T$的输入矩阵为$X_T=\{y_1,y_2,\cdots,y_T\} \in \mathbb{R}^{n\times T}$。

在大多数情况下，预测工作的范围是根据环境的需要而选择的，例如:交通流量方面，预测工作的范围由小时至一天;对于股票市场数据，即使提前几秒或几分钟的预测对于产生回报也是有意义的。

图2展示了所提议的LSTnet架构的概述。LSTNet是一个深度学习框架，专门为长短期模式混合的多变量时间序列预测任务设计。在接下来的几节中，我们将详细介绍LSTNet的构建块。

### 3.2 卷积组件

LSTNet的第一层是一个没有池化的卷积网络，它的目标是提取时间维中的短期模式以及变量之间的局部依赖关系。卷积层由多个宽$w$高$n$的 filters (高度设置为相同数量的变量)组成。第$k$个 filter 扫过输入矩阵X并产生

$$
h_k = RELU(W_k * X + b_k)
$$

其中$*$表示卷积操作和输出$h_k$是一个向量，而$RELU$函数是$RELU(x) = \max(0,x)$。我们通过在输入矩阵$X$的左边填充0使每个$h_k$的长度为$T$。卷积层的输出矩阵大小为$d_c \times T$，其中$d_c$为 filters 数量。

### 3.3 递归组件

卷积层的输出同时输入到递归组件和递归跳越组件(在3.4小节中进行描述)。递归组件是一个带有门控递归单元（GRU)[6]的递归层，使用$RELU$函数作为隐藏的更新激活函数。$t$时刻递归单位的隐状态计算为:

$$
r_t = \sigma(x_tW_{xr}+h_{t-1}W_{hr}+b_r) \\\\
u_t = \sigma(x_tW_{xu}+h_{t-1}W_{hu}+b_u) \\\\
c_t = RELU(x_tW_{xc}+r_t \odot(h_{t-1}W_{hc})+b_c) \\\\
h_t = (1-u_t)\odot h_{t-1}+u_t\odot c_t
$$

其中$\odot$是按元素乘，$\sigma$是 sigmoid 函数，$x_t$是这一层在时刻$t$的输入。这一层的输出是每个时间戳的隐状态，虽然研究人员倾向于使用 tanh 函数作为隐藏更新激活函数，但我们从经验上发现 RELU 的性能更可靠，通过它梯度更容易反向传播。

### 3.4 递归跳跃组件

带有GRU[6]和LSTM[15]单元的递归层经过精心设计，以记住历史信息，从而了解相对长期的依赖关系。然而，由于梯度消失，GRU和LSTM在实际应用中往往不能捕捉到非常长期的相关性。我们建议通过一种新的递归跳跃组件来缓解这个问题，该组件利用了真实集合中的周期性模式。例如，每天的用电量和交通使用量都呈现出明显的规律。如果我们要预测今天t点的用电量，季节预测模型中的一个经典技巧是除了除了最近的记录外还利用历史日t点的记录。由于一个周期(24小时)的长度非常长，以及随后的优化问题，这种类型的依赖关系很难被现成的循环单元捕获。受此技巧的启发，我们开发了一个具有时间跳跃连接的递归结构，以扩展信息流的时间跨度，从而简化优化过程。具体地，在当前的隐藏单元与相邻周期的同一相位的隐藏单元之间添加跳转链接。更新过程可以表述为：

$$
r_t = \sigma(x_tW_{xr}+h_{t-p}W_{hr}+b_r) \\\\
u_t = \sigma(x_tW_{xu}+h_{t-p}W_{hu}+b_u) \\\\
c_t = RELU(x_tW_{xc}+r_t \odot(h_{t-p}W_{hc})+b_c) \\\\
h_t = (1-u_t)\odot h_{t-p}+u_t\odot c_t
$$

其中，该层的输入为卷积层的输出，$p$为跳过的隐藏单元数。对于具有明确周期模式的数据集(例如，每小时电力消耗和交通使用数据集的$p = 24$)，可以很容易地确定$p$的值，否则必须进行调优。在我们的实验中，我们根据经验发现，即使是后一种情况，调优后的$p$也可以显著地提高模型的性能。此外，可以很容易地扩展LSTNet以包含跳跃长度$p$的变体。

我们使用一个稠密层来合并递归和递归跳跃组件的输出。稠密层的输入包括$t$时刻递归组件的隐藏状态，记为$h_t^R$, $t-p+1$时刻到$t$时刻的递归跳跃组件的$p$个隐藏状态，记为$h_{t-p+1}^S,h_{t-p+2}^S,\cdots,h_{t}^S$。密层的输出计算为:

$$
h_t^D = W^Rh_t^R + \sum_{i=0}^{p-1}W_i^Sh_{t-i}^S+b
$$

其中$h_t^D$是图2中上半部分神经网络在$t$时刻的预测结果。

### 3.5 时间注意力层

然而，递归跳跃层需要一个预定义的超参数$p$，这对于非季节性的时间序列预测是不利的，或者它的周期长度是随时间变化的。为了解决这个问题，我们考虑了另一种方法，即注意力机制[1]，它学习输入矩阵每个窗口位置的隐藏表征的加权组合。具体来说,在时刻$t$的关注力权重$\alpha_t \in \mathbb{R}^q$计算为：

$$
\alpha_t = AttnScore(H_t^R,h_{t-1}^R)
$$

其中$H_t^R = [ h_{t-q}^R,\cdots,h_{t-1}^R ]$是一个矩阵，它巧妙地将RNN的隐藏表征按列堆叠起来，而AttnScore是一些相似函数，比如点积、余弦或由一个简单的多层感知器参数化。

时间注意力层最终的输出是加权上下文向量$c_t=H_t\alpha_t$和最后窗口隐藏表征$h_{t-1}^R$的拼接，连上线性投影运算：

$$
h_t^D = W[c_t;h_{t-1}^R]+b
$$

### 3.6 自回归组件

由于卷积和递归组件的非线性特性，神经网络模型的一个主要缺点是输出的比例对输入的比例不敏感。遗憾的是，在具体的真实数据集中，输入信号的尺度会以非周期性的方式不断变化，这大大降低了神经网络模型的预测精度。在第4.6节中给出了一个失败的具体例子。为了解决这一缺陷，我们将LSTNet的最终预测分解为一个线性部分(主要关注局部尺度问题)和一个包含重复模式的非线性部分，这在本质上与公路网[29]类似。在LSTNet体系结构中，我们采用经典的自回归(AR)模型作为线性分量。Ln表示AR分量的预测结果ht∈R, thearmodelaswar∈Rqar, bar∈R，其中，qar是输入矩阵上输入窗口的大小。注意，在我们的模型中，所有维都共享相同的一组线性参数。AR模型表示如下:

---
**参考**：
1. 论文：Guokun Lai, Wei-Cheng Chang, Yiming Yang, Hanxiao Liu [Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks](https://arxiv.org/abs/1703.07015)
